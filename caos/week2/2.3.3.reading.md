# **Cache Types**

* **i-cache (Instruction cache)**: Stores instructions only; read-only, simpler design, optimised for instruction access patterns.
* **d-cache (Data cache)**: Stores program data only; optimised for data access patterns.
* **Unified cache**: Stores both instructions and data; more flexible but can lead to *conflict misses* between data and instruction fetches.
* **Modern approach**: Separate L1 i-cache and d-cache for simultaneous access (one instruction + one data per cycle), different block sizes/associativities.

### **Intel Core i7 Example**

* **Each core**: Private L1 i-cache (32KB, 8-way), L1 d-cache (32KB, 8-way), L2 unified cache (256KB, 8-way).
* **Shared**: L3 unified cache (8MB, 16-way).
* **All SRAM caches** are on-chip ‚Üí faster access, reduced latency.
* **Access times**: L1 ‚âà 4 cycles, L2 ‚âà 10 cycles, L3 ‚âà 40‚Äì75 cycles.

---

## **Cache Performance Metrics**

* **Miss Rate** = `misses / total references`
* **Hit Rate** = `1 - miss rate`
* **Hit Time** = Time to return data from cache (incl. tag lookup, block selection).
* **Miss Penalty** = Extra time when data is not in cache (depends on source: L2 ‚âà 10 cycles, L3 ‚âà 50 cycles, main memory ‚âà 200+ cycles).

---

## **Factors Affecting Cache Design**

### 1. **Cache Size**

* Larger cache ‚Üí higher hit rate but longer hit time (harder to make large memory fast).
* Hierarchical design: L1 < L2 < L3 size.

### 2. **Block Size**

* Larger block ‚Üí better spatial locality usage but fewer total blocks ‚Üí worse temporal locality performance.
* Larger block = higher miss penalty (more transfer time).
* Modern systems (e.g., Core i7): 64B blocks as compromise.

### 3. **Associativity (E)**

* Higher associativity ‚Üí fewer *conflict misses* but higher cost:

  * More tag bits, LRU tracking, complex replacement logic.
  * Can increase both hit time & miss penalty.
* Typical: L1/L2 = 8-way, L3 = 16-way.

### 4. **Write Strategy**

* **Write-through**: Simpler, immediate write to memory; can use independent write buffer; higher bandwidth use.
* **Write-back**: Writes only on eviction; reduces memory traffic, better for deeper cache levels.

---

## **Cache Terminology**

* **Block**: Fixed-size data unit moved between memory & cache.
* **Line**: Storage slot in cache holding one block + metadata (valid bit, tag, etc.).
* **Set**: Collection of cache lines that a memory block can map to.
* In direct-mapped caches: 1 line per set; in set-associative: multiple lines per set.

---

## **Writing Cache-Friendly Code**

### **Principles**

1. **Make the common case fast** ‚Äì Optimise inner loops in hot functions.
2. **Reduce misses in inner loops** ‚Äì Better miss rate = faster execution.

### **Locality Types**

* **Temporal locality**: Reuse of recently accessed data (keep hot variables in registers).
* **Spatial locality**: Access data in contiguous memory regions (stride-1 patterns).

### **Good Practices**

* **Stride-1 access**: Access elements sequentially; fills cache lines efficiently.
* **Row-major traversal** (in C): Matches memory layout, minimises misses.
* Avoid column-major iteration on large arrays (thrashes cache if array > cache size).

### **Impact Example**

* **sumarrayrows** (row-major) runs \~25√ó faster than **sumarraycols** (column-major) for large arrays due to fewer misses.

---

‚úÖ **Key Takeaways**:

* Separate i-cache and d-cache improve parallelism and avoid instruction/data conflicts.
* Cache parameters (size, block size, associativity, write policy) involve trade-offs between hit rate, hit time, and miss penalty.
* Writing cache-friendly code means structuring memory access patterns to exploit spatial & temporal locality.
* Poor locality can drastically degrade performance, especially in large data structures.

# üèîÔ∏è The Memory Mountain ‚Äì Cache Performance in Action

## üìå Key Concepts

* **Read throughput (read bandwidth):**
  How fast a program can read from memory.

  * Formula: bytes read √∑ time (s).
  * Measured in MB/s or GB/s.
* **Program trace experiment:**
  Reads elements of an array with varying *size* (working set size) and *stride* (step between elements).

  * **Size ‚Üí Temporal locality** (small = better reuse).
  * **Stride ‚Üí Spatial locality** (small = better sequential access).
* **Memory Mountain:**
  2D plot of throughput vs. temporal + spatial locality. Every computer has a unique ‚Äúmountain‚Äù shape.

---

## üñ•Ô∏è Anatomy of the Mountain

* **Ridges (temporal locality):**
  Correspond to data fitting entirely in **L1, L2, L3, or main memory**.

  * L1 peak ‚Üí \~14 GB/s
  * Main memory valley ‚Üí \~900 MB/s
  * Over *10√ó* difference depending on locality.

* **Slopes (spatial locality):**
  Throughput decreases as stride increases (worse spatial locality).

  * At stride = block size (64B here), every read is a miss ‚Üí throughput limited by transfers from lower cache.

* **Flat ridge at stride = 1:**
  Due to **hardware prefetching** ‚Äì CPU automatically pulls in sequential blocks before needed.

---

## üìä Examples from Intel Core i7 Haswell

* L1 cache: 32 KB ‚Üí \~12‚Äì14 GB/s peak
* L2 cache: 256 KB ‚Üí drops slightly but still multi-GB/s
* L3 cache: 8 MB ‚Üí flatter but lower
* Main memory: far slower, 0.9‚Äì1.0 GB/s

---

## ‚ö° Locality Insights

* **Temporal locality (reuse over time):**
  Keep working set small ‚Üí more reuse from faster caches.
* **Spatial locality (use neighbors):**
  Sequential access exploits cache blocks + prefetching.
* Even with poor temporal locality, **spatial locality rescues performance** (sequential > scattered).

---

## üõ†Ô∏è Programmer‚Äôs Takeaways

* Structure code to **live in the peaks**:

  * Reuse data (temporal).
  * Access sequentially (spatial).
* **Avoid valleys** where cache misses dominate.
* **Cache-friendly programming** = loop order, stride-1 access, blocking techniques.

---

‚úÖ **Summary:**
The Memory Mountain shows that performance isn‚Äôt a single number but a landscape shaped by cache hierarchy, locality, and access patterns. Programmers who exploit temporal + spatial locality ride the peaks (fast cache hits), while poor code patterns fall into valleys (slow memory accesses).
