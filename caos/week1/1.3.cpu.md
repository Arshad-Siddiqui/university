# CPU!

## âœ… Key Terminology and Concepts

| **Term**                                   | **Definition**                                                                                                                         |
| ------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------- |
| **CPU (Central Processing Unit)**          | A chip made of millions of transistors that performs arithmetic, logical, and control tasks based on instructions fetched from memory. |
| **Microprocessor**                         | A compact, integrated version of the CPU on a single chip, central to all modern computing devices.                                    |
| **Register**                               | Small, fast storage units inside a CPU for holding data temporarily during execution.                                                  |
| **Instruction**                            | A command that the CPU executes; could be arithmetic, logic, or control-based.                                                         |
| **Fetch-Execute Cycle**                    | The repeated process where a CPU reads an instruction from memory and performs the required operation.                                 |
| **Power Density**                          | The amount of electrical power consumed per unit volume of a processor; higher density = more heat.                                    |
| **Dynamic Power**                          | Power consumed when transistors switch on/off during operation.                                                                        |
| **Static Power**                           | Power consumed by transistors even when idle (due to leakage).                                                                         |
| **Technology Node**                        | A measure of transistor size in nanometers (e.g., 10nm); smaller nodes allow more transistors on a chip.                               |
| **MIPS (Million Instructions Per Second)** | A basic performance metric for how many instructions a CPU can handle per second.                                                      |
| **CPI (Clocks Per Instruction)**           | Average number of clock cycles a processor needs to execute an instruction.                                                            |
| **Branch Prediction**                      | A method where the CPU predicts the outcome of a decision in code to avoid stalling.                                                   |
| **Speculative Execution**                  | A performance strategy where the CPU executes instructions before knowing if they are needed.                                          |
| **Superscalar Architecture**               | CPU design that allows multiple instructions to be executed per clock cycle.                                                           |
| **Multicore Processor**                    | A chip that includes multiple CPU cores to increase parallelism and processing power.                                                  |

---

## ðŸ” Question 1: How do heat and power constraints influence processor design and performance? What trade-offs do engineers face?

### ðŸ“Œ Influence of Heat and Power:

* **Every transistor switching on/off consumes power**, generating heat as a by-product.
* As processors shrink (smaller technology nodes), **more transistors can be used**, but **this increases total power consumption** and **thermal output**.
* Modern CPUs require **complex cooling systems** (heat sinks, fans, sometimes liquid cooling).
* **Mobile and embedded devices** are **thermally constrained**, meaning performance is often limited not by processor capability but by how much heat the system can dissipate.

### âš–ï¸ Trade-Offs:

* **Performance vs. Power**: Higher clock speeds and more transistors increase performance but lead to more power consumption and heat.
* **Design Complexity**: To manage heat, CPUs must include **dynamic scaling**, **low-power states**, and **throttling**, adding to complexity.
* **Cost vs. Efficiency**: Systems like data centers must balance electricity costs against performance; efficient processors may cost more up-front but save in operating costs.
* **Physical Limits**: Engineers must deal with **diminishing returns**â€”increasing complexity doesnâ€™t always yield proportional performance gains.

---

## ðŸ” Question 2: In what ways can increasing clock frequency help or hinder processor performance? How do modern systems adapt?

### âœ… Benefits of Increasing Clock Frequency:

* **Faster execution**: More instructions can be processed per second.
* **Lower latency**: Individual operations complete faster.
* **Improves raw performance**, especially for single-threaded tasks.

### âŒ Downsides and Limitations:

* **Increased power consumption** (dynamic power scales with frequency).
* **More heat generation**, leading to thermal throttling or the need for better cooling.
* **Not linearly scalable**: Performance gains often plateau due to other bottlenecks like memory access or instruction dependencies.

### ðŸ›  How Modern Systems Adapt:

1. **Clock Scaling (Throttling)**:

   * Dynamic adjustment of frequency depending on workloadâ€”reduces power and heat during low demand.
   * Known by terms like *SpeedStep*, *Turbo Boost*, *sleep modes*.

2. **Parallelism**:

   * **Multicore processors**: Spread workload across cores instead of ramping up single-core speed.
   * **Superscalar architecture**: Execute multiple instructions per cycle.

3. **Efficient Architectures**:

   * **Pipeline and out-of-order execution** reduce idle cycles.
   * **Branch prediction and speculative execution** maintain high throughput despite non-linear code paths.

4. **Specialization**:

   * Some CPUs include **low-power cores** for background tasks (e.g., ARM big.LITTLE architecture).

---

Would you like a printable summary or a flashcard-style breakdown of these terms and concepts?

# 1.3.2 reading

## ðŸ”§ 3.4 Internal CPU Organisation

### **ALU (Arithmetic Logic Unit)**

* Performs arithmetic and logic operations like addition, subtraction, AND, OR, etc.
* Operates on data fetched from registers or memory.

### **Control Unit**

* Directs the operations of the processor by sending control signals.
* Coordinates fetch, decode, and execute cycles.
* Uses **microinstructions** to carry out each CPU operation.

### **Registers**

* Small, fast storage locations within the CPU.
* Temporarily store data and instructions.
* Common types:

  * **Accumulator (AC):** Holds intermediate results.
  * **Program Counter (PC):** Keeps track of the address of the next instruction.
  * **Instruction Register (IR):** Holds the currently executing instruction.
  * **Memory Address Register (MAR):** Stores addresses to be accessed in memory.
  * **Memory Buffer Register (MBR):** Stores data read from or written to memory.

---

## ðŸ§  3.5 Microinstructions and Microsequences

### **Microinstruction**

* A low-level command that activates a specific part of the processor's hardware.
* One machine instruction is often composed of **multiple microinstructions**.

### **Microsequence**

* A **series of microinstructions** that implement a full machine-level instruction.
* Stored in **control memory** within the control unit.
* Allows complex instructions to be broken into manageable steps (e.g., loading data, performing ALU operations, storing results).

### **Control Memory**

* Stores predefined microsequences.
* Enables flexible and programmable CPU behavior.

---

## â–¶ï¸ 3.6 Instruction Execution

### **Fetch-Decode-Execute Cycle**

1. **Fetch:**

   * Control unit fetches the next instruction from memory using the **PC**.
   * The instruction is loaded into the **IR**.
   * PC is incremented to point to the next instruction.

2. **Decode:**

   * The instruction in IR is interpreted.
   * Control unit identifies what action is needed.

3. **Execute:**

   * The control unit issues **microinstructions** to trigger ALU, memory, or I/O operations.
   * Data may be moved between registers, operated on, or stored in memory.

### **Interrupt Handling**

* CPU can pause the current program to respond to high-priority events (like I/O or errors).
* Saves the current state (PC, etc.) and jumps to an interrupt service routine.

---

## ðŸ’» 3.7 Program Examples (from control logic)

* Demonstrates how complex instructions are constructed from sequences of microinstructions.
* Example: A **LOAD instruction** might involve:

  1. Fetching the operand address.
  2. Accessing memory at that address.
  3. Moving the value to a register (e.g., the Accumulator).

---

## ðŸ“š Key Terminology

| Term                     | Meaning                                                           |
| ------------------------ | ----------------------------------------------------------------- |
| **ALU**                  | Executes arithmetic/logical operations on data.                   |
| **Control Unit**         | Orchestrates CPU operations by sending control signals.           |
| **Register**             | Fast internal memory for temporary storage of data and addresses. |
| **Microinstruction**     | A single hardware-level action initiated by the control unit.     |
| **Microsequence**        | A sequence of microinstructions for one full machine instruction. |
| **Control Memory**       | Stores all microsequences; often programmable (firmware).         |
| **Program Counter (PC)** | Holds address of the next instruction to fetch.                   |
| **Instruction Register** | Holds the currently executing instruction.                        |
| **MAR** / **MBR**        | Work together to access memory addresses and data.                |
| **Interrupt**            | A signal that diverts CPU to a higher-priority task.              |

---

# ðŸ“ˆ 2.2 Designing for Performance

## âš™ï¸ Overview

* Computers are getting faster and cheaper, leading to complex apps (e.g., speech recognition, multimedia, simulations).
* Despite architectural evolution, modern systems still use basic structures similar to the original IAS computer.
* The challenge is not just speed, but feeding data fast enough to keep the processor working efficiently.

---

## ðŸš€ Microprocessor Speed

* **Mooreâ€™s Law** continues to drive exponential growth in chip density and performance (4x more transistors every 3 years).
* **Problem**: The processor can handle more than itâ€™s being fed. This leads to underutilisation unless smarter techniques are used.

### ðŸ”§ Techniques to Utilise CPU Potential:

1. **Pipelining**: Multiple instruction stages processed simultaneously (like an assembly line).
2. **Branch Prediction**: Processor anticipates which instruction path to follow and preloads them.
3. **Data Flow Analysis**: Instructions are reordered to execute as soon as their data is ready.
4. **Speculative Execution**: Executes instructions ahead of time (based on prediction) and saves results temporarily.

---

## âš–ï¸ Performance Balance

* **Issue**: Memory and I/O havenâ€™t scaled as fast as processors. This mismatch leads to the processor waiting ("stalling").

### ðŸ§  Solutions for Memory Bottleneck:

* **Wider Memory Buses**: Retrieve more bits at once.
* **Efficient DRAM Interface**: Includes on-chip caches/buffers.
* **Better Caching**: L1, L2, sometimes L3 caches placed between CPU and main memory (on-chip for speed).
* **Hierarchy of Buses**: Uses high-speed buses and multiple layers to increase bandwidth.

### ðŸ§± I/O Solutions:

* **Faster Interconnects**: High-speed buses and structures to manage data from modern peripherals.
* **Multiprocessing**: Multiple CPUs help meet high I/O demands.

### ðŸ” Key Idea: **System Balance**

Balance performance across:

* Processor speed
* Memory speed and access
* I/O bandwidth
* Software expectations and access patterns

---

## ðŸ”© Improvements in Chip Organization

### â¬†ï¸ Ways to Boost Speed:

1. **Faster Hardware**: Smaller gates = higher clock rates.
2. **Larger, Faster Caches**: Especially on-chip.
3. **Improved Architecture**: More parallelism in how instructions are executed.

### âš ï¸ Barriers to Scaling Clock Speed:

* **Power Dissipation**: More heat with more transistors and faster clocks.
* **RC Delay**: Resistance Ã— Capacitance = slower interconnects.
* **Memory Latency**: Memory still lags behind CPU speed.

### ðŸ“Š Trends:

* Clock speeds are plateauing due to thermal/power issues.
* Focus shifting from speed increases to **architectural improvements**.

---

## â›“ï¸ Parallelism Techniques:

1. **Pipelining**: Multiple instruction stages happen in parallel.
2. **Superscalar**: Multiple pipelines allow several instructions to be executed at once.

ðŸŸ¡ **Limitations**:

* Diminishing returns: Caches and pipelining becoming maxed out.
* Need a new strategy...

---

# ðŸ§  2.3 Multicore, MICs, and GPGPUs

## ðŸ§© Multicore Processors

* **Definition**: Multiple CPU cores on a single chip sharing cache and interconnects.
* **Goal**: Increase performance *without* increasing clock rate.
* **Benefit**: Doubling cores â‰ˆ doubling performance (if software supports it).
* **Trend**: From 2 to 4, 8, 16... to 50+ cores.

### ðŸ’¡ Cache Hierarchy:

* **L1**: Dedicated per core (fastest)
* **L2/L3**: Shared among cores (slightly slower)

---

## ðŸ§± MIC (Many Integrated Core)

* **Next Gen Multicore**: 50+ cores per chip.
* Optimised for highly parallel workloads (scientific computing, simulations).
* Focus is on power efficiency and simplicity rather than complex single-core design.

---

## ðŸŽ® GPGPU (General Purpose GPU)

* **GPU vs CPU**:

  * GPU: Great at handling **parallel operations** (ideal for graphics, video, scientific computation).
  * CPU: General-purpose, better at sequential logic and control.

* **GPGPU**: Using GPU for general-purpose computing (not just graphics). Popular in AI, simulation, data processing.

* **Hybrid Chips**: Combining CPUs, GPUs, and special-purpose cores for efficient workload distribution.

---

## ðŸ”‘ Summary of Concepts & Terms

| Term                      | Definition                                                                      |
| ------------------------- | ------------------------------------------------------------------------------- |
| **Pipelining**            | Instruction steps are executed in overlapping stages, like an assembly line.    |
| **Branch Prediction**     | Guesses which way a branch (e.g., if-else) will go to preload instructions.     |
| **Data Flow Analysis**    | Identifies dependencies between instructions to optimise execution order.       |
| **Speculative Execution** | Executes instructions ahead of time using predictions.                          |
| **Superscalar**           | Multiple instruction pipelines allow more parallelism.                          |
| **Cache**                 | Small, fast memory that stores recently used data to reduce memory access time. |
| **Multicore**             | Multiple processor cores on one chip, each capable of independent execution.    |
| **MIC**                   | Many Integrated Core; 50+ simpler cores used for parallel tasks.                |
| **GPGPU**                 | Using GPU for non-graphics computing via parallel data processing.              |

---

## ðŸŒŠ General Principles of Pipelining

Pipelining is a performance enhancement technique where instruction execution is split into stages. Each stage does part of the task, allowing multiple instructions to be processed simultaneously.

### ðŸ“Œ Key Concepts

* **Pipelining** increases **throughput** (instructions per second) but may **increase latency** (time for one instruction).
* Inspired by real-world pipelines like:

  * A **cafeteria line**: salad â†’ main â†’ dessert â†’ drink
  * A **car wash**: rinse â†’ soap â†’ scrub â†’ wax â†’ dry
* In a **non-pipelined system**, each task must fully complete before the next starts.
* In a **pipelined system**, instructions overlap in execution stages.

---

## ðŸ§  Computational Pipelines

Instructions are the â€œcustomersâ€ moving through pipeline stages (e.g., Fetch, Decode, Execute...).

### â± Example (Unpipelined System)

* Takes 300 ps for logic computation, plus 20 ps to store result.
* **Total latency** = 320 ps
* **Throughput** = 1 instruction per 320 ps = **\~3.12 GIPS** (giga instructions/sec)

### âš™ï¸ Example (3-Stage Pipelined System)

* Split into stages A, B, and C (100 ps each + 20 ps for pipeline register delay)
* **Clock cycle** = 120 ps (longest stage delay + register)
* **Throughput** = 1 instruction per 120 ps = **\~8.33 GIPS**
* **Latency** = 3 Ã— 120 ps = **360 ps**
* **Speedup** = 8.33 / 3.12 â‰ˆ **2.67Ã—** faster than unpipelined

ðŸ§ª **Trade-off**: Pipelining **increases throughput** at the cost of **slightly higher latency** and **added complexity** (pipeline registers).

---

## â³ Detailed Pipeline Timing

* Clock signal controls when data moves to the next stage (on the **rising edge**).
* Each pipeline stage processes its part of the instruction **simultaneously**.
* Registers only update when the clock rises, avoiding signal overlap or premature state changes.

ðŸš¨ If the **clock is too fast**, the combinational logic may not finish processing, leading to invalid results.

---

## ðŸª« Limitations of Pipelining

### ðŸ”º Nonuniform Partitioning

* Ideal: Each stage takes equal time â†’ uniform 120 ps clock cycle
* Real: Different stages may have different delays (e.g., 150 ps, 100 ps, 50 ps)

  * Slowest stage (150 ps) dictates clock speed
  * **New clock cycle** = 150 + 20 = 170 ps
  * **Throughput** = 1 instruction per 170 ps = **\~5.88 GIPS**
  * **Latency** = 3 Ã— 170 = **510 ps**
* ðŸ’¡ **Only the slowest stage is fully utilised**, others idle

### ðŸ—ï¸ Hardware Limitations

* Some components (ALU, memory) cannot easily be split into smaller units
* This makes it harder to balance stage delays, which is crucial for efficiency

---

## ðŸ§© Practice Case (Simplified Summary)

Given delays: A=80ps, B=30ps, C=60ps, D=50ps, E=70ps, F=10ps
Goal: Insert registers (each adds 20 ps delay) to optimise throughput

> You must decide **where** to place pipeline registers to balance stage lengths as much as possible while minimising the **longest stage time**, which becomes the limiting factor for the clock speed.

---

## ðŸ”‘ Takeaways

* Pipelining increases throughput, not necessarily speed per instruction
* Design challenges include balancing stage delays and handling control flow (not covered here)
* Proper pipelining can boost performance significantly, but poor implementation can bottleneck the system