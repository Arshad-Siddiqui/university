# Advanced CPU Architecture and OS Support

Modern processors are no longer just single scalar machines. To handle increasing workloads efficiently, they incorporate **superscalar execution, multicore design, and hyperthreading (SMT)**. The OS must adapt to support these features.

---

## From Scalar to Superscalar

* **Scalar execution** → executes one instruction per clock cycle.
* **Instruction-Level Parallelism (ILP):** many instructions in code are independent and can be executed in parallel.
* **Superscalar processors** issue and execute multiple instructions per clock cycle, exploiting ILP.
* **Limits of ILP:** Compilers can reorder instructions, but typically only **2–4 instructions per cycle** execute in parallel. Beyond that, complexity and diminishing returns make further ILP costly in power/heat.

---

## Multicore

* Instead of pushing ILP further, designers duplicated processor cores → **Thread-Level Parallelism (TLP).**
* **Multicore chips**: usually 4–8 powerful cores in consumer CPUs (datacenter chips may have dozens).
* **OS role:** Scheduler can now run multiple tasks *truly concurrently* on separate cores. Each core may even have its own scheduling queue.
* **Shared resources:** All cores share the memory bus and I/O. This can be a bottleneck.
* **Cache hierarchies:** Per-core caches (L1, L2) and a shared L3 help reduce memory contention.
* **Performance gain:** In theory, n cores → n× performance, but in reality less (limited by memory and OS overhead). This links to **Amdahl’s Law**.

---

## Hyperthreading & Simultaneous Multithreading (SMT)

* **Problem:** Context switches are expensive (hundreds of cycles) because a core must save and restore registers, buffers, pipeline state, etc.
* **Solution:** Duplicate the *processor state* (register sets, etc.) so multiple threads can reside in hardware simultaneously, but share execution units.
* **Effect:**

  * If one thread stalls (e.g., waiting on memory), another can use the idle execution units.
  * Improves utilisation of CPU resources.
  * Each thread progresses in small steps, interleaved with others.
* **Performance:**

  * Does not give 2× (or 4×) performance for 2-way/4-way hyperthreading.
  * Often gives \~20–30% improvement depending on workload.
  * Best case is when threads complement each other (e.g., one is compute-bound, the other is memory-bound).

---

# Key Takeaways

* **Superscalar execution**: Exploits ILP, but limited.
* **Multicore processors**: Exploit TLP, running multiple tasks at once.
* **Hyperthreading/SMT**: Improves CPU utilisation by interleaving instructions from multiple threads.
* **OS Scheduling Impact**: Must recognise multiple cores and hyperthreads, distributing workload effectively.
* **Performance Reality**: Gains are substantial but limited by shared resources and OS overhead.

---

# Exam Question Links

**Q: How do multicore and hyperthreading reduce task-switching overhead in uniprocessors?**

* Multicore removes much of the illusion of multitasking by allowing *real parallelism*—multiple tasks run without switching.
* Hyperthreading reduces wasted CPU cycles by keeping multiple threads resident in hardware, avoiding costly full context switches.

**Q: How does OS design differ between uniprocessor and multiprocessor systems?**

* **Uniprocessor OS:** Must rely heavily on time-slicing and scheduling fairness.
* **Multiprocessor OS:** Must manage shared memory, cache coherence, and distribute tasks across cores. Balancing load and minimising contention becomes central.

---

✨ In short: The shift from **scalar → superscalar → multicore → SMT** reflects how hardware and operating systems co-evolve: each new design demands smarter scheduling, memory handling, and task management to extract performance without wasting resources.