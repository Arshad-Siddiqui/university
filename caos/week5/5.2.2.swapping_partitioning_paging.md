# Reading of Computer Organization and architecture section 8.3

Link to [site](https://eu.alma.exlibrisgroup.com/leganto/public/44YORK_INST/citation/43044322080001381?auth=SAML)

# Memory Management

Efficient memory management is crucial in multiprogramming systems to keep the CPU busy while many processes compete for limited memory.

---

## Swapping

* **Idea:** Processes can be swapped in and out of memory to/from disk when idle or waiting for I/O.
* This prevents the CPU from going idle if all processes in memory are blocked.
* **Trade-off:** Swapping itself is an I/O operation (slow), but disk is still the fastest form of secondary storage compared to tape/printers, so it helps.
* **Intermediate Queue:** Holds suspended processes swapped out of memory.
* Sets the stage for **virtual memory** which improves on this model.

---

## Partitioning

Memory can be subdivided to allocate space for multiple processes.

### Fixed-size partitions

* Simple but inefficient.
* Equal or unequal partitions.
* Leads to **internal fragmentation** (unused space inside partitions).

### Variable-size partitions

* Processes are given exactly the space they need.
* Leads to **external fragmentation** (scattered holes too small to fit new processes).
* **Compaction** can occasionally reorganise memory to eliminate holes, but itâ€™s costly.

---

## Logical vs. Physical Addresses

* **Logical address:** Relative position within a program (used in code).
* **Physical address:** Actual location in RAM.
* CPU uses a **base register** to translate logical â†’ physical at runtime.
* This allows processes to move around in memory without breaking their execution.

---

## Paging

* Memory divided into **fixed-size frames**.
* Process divided into **equal-sized pages**.
* OS uses a **page table** per process to map pages â†’ frames.
* Eliminates fragmentation issues from partitioning.
* CPU hardware automatically translates **(page number, offset)** â†’ physical address.

---

## Virtual Memory

* Extension of paging with **demand paging**: only load pages into RAM when needed.
* Unused pages remain on disk until referenced.
* If a referenced page isnâ€™t in memory â†’ **page fault**, OS loads it.
* **Benefits:**

  * Only a small working set of a process needs to be in memory.
  * Many more processes can run concurrently.
  * Process size can exceed physical RAM.

### Problems

* **Thrashing:** Too many page faults â†’ CPU spends more time swapping than executing.
* Requires clever **page replacement algorithms** (e.g., LRU or approximations).

---

## Page Table Structures

* **Single-level page table:** Maps virtual â†’ physical. Can grow huge.
* **Two-level page table:** Reduces memory required for large processes.
* **Inverted page table:** One entry per frame, not per page. Uses hashing to map virtual â†’ physical efficiently.

---

## Translation Lookaside Buffer (TLB)

* Small, fast cache for page table entries.
* Stores most recently used mappings.
* Exploits **locality** so most memory references hit in the TLB, avoiding slow page table lookups.
* Without TLB, each memory access could require **two RAM lookups** (one for page table, one for data).

---

# Key Takeaways

* **Swapping**: First step toward handling more processes than RAM allows.
* **Partitioning**: Fixed = simple but wasteful, variable = flexible but fragmented.
* **Paging**: Solves fragmentation, forms basis of virtual memory.
* **Virtual Memory**: Extends RAM with disk; enables larger and more concurrent processes.
* **TLB**: Makes virtual memory practical by avoiding slow address translations.

---

âœ… **Q1: How do swapping and paging help OS balance memory?**

* Swapping increases CPU utilisation by bringing in new processes when others are blocked.
* Paging ensures efficient use of physical memory without fragmentation.
* Together they allow more processes to run concurrently in limited memory.

âœ… **Q2: How does virtual memory enhance system performance, and what are the trade-offs?**

* Gives illusion of vast memory, enabling larger programs and better multiprogramming.
* Trade-off: Page faults and thrashing can degrade performance if not managed properly.
* Needs hardware support (MMU, TLB) and smart replacement algorithms.

Great set of questions ðŸ‘Œ Letâ€™s untangle paging and frames step by step:

---

## Paging vs Frames

* **Paging (process side):**

  * A process is divided into equal-sized chunks called **pages**.
  * These pages exist in the **logical address space** (the programâ€™s view of memory).

* **Frames (memory side):**

  * Physical memory (RAM) is divided into equal-sized chunks called **frames**.
  * A page fits exactly into a frame (same size).
  * The OS maintains a **page table** to map each process page â†’ physical frame.

**Mnemonic:**
ðŸ‘‰ *Page = program side*
ðŸ‘‰ *Frame = hardware memory side*

---

## Why it helps

Without paging, if you try **variable partitioning**:

* Process A = 10 MB, Process B = 20 MB â†’ must carve memory into chunks of that size.
* Over time, removing/adding processes leaves little "holes" (external fragmentation).

Paging fixes this:

* Every chunk (page/frame) is the same size, e.g. 4 KB.
* Any free frame can hold any page.
* No external fragmentation (no scattered unusable gaps).

---

## Do we still waste memory? (Yes, a little)

* Inside the last page of a process, the program might not use all the space.
* Example: Page size = 4 KB, but the program needs only 3.7 KB â†’ 0.3 KB wasted.
* This is **internal fragmentation** (wasted space *inside* a page).

ðŸ‘‰ But itâ€™s usually small compared to the **external fragmentation** of variable partitions. Thatâ€™s why paging is considered a better trade-off.

---

## Quick Recap

* **Pages = process chunks** (logical address space).
* **Frames = physical memory chunks** (RAM side).
* **Mapping:** Each page goes into a frame (via the page table).
* **Fragmentation:**

  * External fragmentation = solved.
  * Internal fragmentation = small leftover waste in the last page.