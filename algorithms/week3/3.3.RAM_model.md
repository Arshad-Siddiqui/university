# RAM Model of Computation and Big O Notation

## Introduction

- Practical way of measuring how relatively long an algorithm will take to run.

- Relies on assuming each operation, such as addition or comparison, takes a similar amount of time to run.

- Key part of analysis is seeing how the performance is effected by input size. This allows you to compare algorithms without needing particular hardware/software as you're just measuring the pattern associated with increasing input size.

- Will also cover big O notation. Just a clear and concise way to convey what input growth does to it.

### 3.3.1 The motivation behind the RAM model

The alternative is called the experimental approach where you test on real world systems to get real world results.

One of the problems with this is that this varies greatly with different hardware structures. Algorithms might compare differently on different hardware.

For instance cache size and memory hierarchy can greatly affect which algorithm comes out on top.

Software can also have an impact as well. Different environments canput different strains on the hardware causing results to perhaps be unique to that particular setup.

#### Overall...

This just makes it tricky to compare algorithms when you have all of these other variables to keep fixed and how you don't know if your results are caused by these fixed variables.

You can't draw universal conclusions just using 1 hardware configuration.

## Reading: The need for using the RAM framework over experimental performance measurement

- RAM: Random Access Machine
- Abstract and standardised allowing easy comparison.

## Benefits of abstracting hardware

- Let's you compare algorithms without considering managing complexity of hardware/software.

- Even if you conduct a 'fair test' it might be just due to the hardware you have chosen.

- Let's you do universal comparisons. There is no hardware to consider. RAM model is very simplified so efficiency is just being compared.

- Let's you use asymptotic analysis. Let's you compare stuff approaching a limit such as infinity.

## Features and properties of the RAM model

Random Access Machine model has this imaginary hardware

- **Memory**: Infinite and flat memory where speed is consistent. This assumes that each memory location is accessible in constant time as opposed to the varying nature found in real life. Such as with cache and virtual memory.
- **Processor**: Executes the code sequentially. Each operation takes the same amount of time.
- **Instruction Set**: Basic, meaning you can load and store values, do math and have conditional branching in the code.
- **Input and Output**: Inputs are predefined and comes from memory and gets returned to memory.

## Limitations of RAM model

- A little unrealistic where uniform access to memory is unlikely. There are levels of cache and disk space.
- Infinite resources is also an unrealistic assumption as often times you're limited by hardware and need to consider a solution that acknowledges this.
---
- This is why it is recommended to consider both. This helps bridge the gap between theory and real world performance.

# 3.3.2 Exporation: 8 Useful Functions for Algorithm Analysis

## Slow Growth Rates

### **Constant** Function

$f$ (n) = c

> c here refers to a constant.

This means no matter how large the input size gets the function takes a constant amount of time.

#### Examples

- Reporting the first element from an array
- Turning an array into 2 smaller subsets by using a single index in the middle.

O(1) in Big O notation

### **Logarithmic** Function

$f$ (n) = log n

> n refers to size of dataset.

As data size grows the amount of extra time required grows slower and slower. log in this context assumes base 2 as computers use binary.

#### Examples

- Binary search: This is where you half the search pool every iteration, meaning you only look at a smaller and smaller portion of the dataset to find your element as the data grows.

## Moderate Growth Rates

### **Linear** Function