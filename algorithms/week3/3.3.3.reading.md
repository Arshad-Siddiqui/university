Here’s a cleaned-up and professional version of your notes, written in polished markdown:

---

# Asymptotic Analysis and Big O Notation

In asymptotic analysis, we focus on how an algorithm’s **performance scales** as the size of its input increases, rather than counting exact operations. We simplify all primitive operations as taking constant time — **O(1)** — and concentrate on growth patterns driven by constructs like loops and recursion.

> 📺 Helpful [video explanation](https://www.youtube.com/watch?v=0oDAlMwTrLo)

---

## What Is an Asymptote?

An **asymptote** is a line a function gets closer and closer to as the input size grows, without ever actually touching it. In algorithm analysis, this corresponds to the **highest-order term** in a function.

Example:

```
f(n) = n² + 2n + 6
```

In Big O analysis, we focus only on the dominant term:

```
f(n) is O(n²)
```

![Asymptote diagram](image-2.png)

Some algorithms may start fast but approach a steep curve as inputs grow. Asymptotic analysis helps us **compare algorithms at scale**.

---

## Big-O Notation (O)

Used to describe an **upper bound** on an algorithm’s growth.

If we say:

```
f(n) ≤ c·g(n) for all n ≥ n₀
```

We write:

```
f(n) is O(g(n))
```

This means that beyond a certain input size `n₀`, `g(n)` is a reliable worst-case growth ceiling for `f(n)`.

![Graph of f(n) vs cg(n)](image-1.png)

---

### How to Use Big O

* **Ignore constants** and **lower-order terms**
* Focus on the dominant factor driving growth

Example:

```
f(n) = 3n⁹ + 9n³ + 12
```

```
f(n) is O(n⁹)
```

Even though `O(n¹⁰⁰)` would also technically be correct, it’s unhelpful — aim to simplify while staying precise.

---

## Other Common Notations

### Ω (Big Omega)

Describes a **lower bound** — how good an algorithm can get.

```
f(n) is Ω(g(n))
```

### Θ (Big Theta)

A **tight bound** — both the upper and lower bounds match.

```
f(n) is Θ(g(n)) ⟺ f(n) is O(g(n)) and f(n) is Ω(g(n))
```

---

## Big O: Limitations

Big O is extremely useful, but **not perfect**. It hides:

* Constant factors
* Lower-order terms

Example:

```
f(n) = 10¹⁰⁰·n → O(n)
g(n) = n² → O(n²)
```

Even though `f(n)` is O(n), in practice it may be worse than `g(n)` for all reasonable `n`.

---

## Constant Time Operations — O(1)

Some operations remain fast **regardless of input size**:

```python
len(data)
```

In Python, `len()` is O(1) because list length is stored.

Accessing an index is also O(1), thanks to contiguous memory:

```text
memory_address + (index × cell_size) = exact element location
```

---

## Prefix Averages (Concept)

Used to calculate the average of all items up to each index in a list.

Useful in time-series analysis, like calculating average returns or moving averages.

---

# 3.4: Justifying Algorithm Correctness

## Counterexamples

To disprove a universal claim, one example is enough.

> Claim: All even numbers are divisible by 4.
> Counterexample: 6 → disproves the claim.

## Contrapositive

A logical technique: prove a statement by proving its **contrapositive**.

> Claim: If `a*b` is even, then `a` or `b` is even.
> Contrapositive: If both `a` and `b` are odd, then `a*b` is odd.

## Contradiction

Assume the opposite of what you want to prove.
If this leads to a contradiction, your original statement must be true.

## 3.4.3: Induction and Loop Invariants

### Induction (Mathematical Induction)

Induction is a **proof technique** used to show that a property holds for all natural numbers (or all numbers beyond a certain base case).

It consists of two steps:

#### 1. **Base Case**

Prove that the statement holds for the initial value, usually `n = 0` or `n = 1`.

#### 2. **Inductive Step**

Assume the statement holds for `n = k` (the **inductive hypothesis**) and prove it must then hold for `n = k + 1`.

> If both steps are valid, the property holds for all integers ≥ base case.

#### Example: Sum of First `n` Natural Numbers

Prove:

```
1 + 2 + ... + n = n(n + 1)/2
```

**Base case (n = 1):**

```
1 = 1(1 + 1)/2 = 1 ✅
```

**Inductive step:**
Assume it’s true for `n = k`:

```
1 + 2 + ... + k = k(k + 1)/2
```

Now prove it for `n = k + 1`:

```
1 + 2 + ... + k + (k + 1)
= k(k + 1)/2 + (k + 1)
= (k(k + 1) + 2(k + 1))/2
= (k + 1)(k + 2)/2 ✅
```

Thus, by induction, the formula holds ∀ `n ≥ 1`.

---

### Strong Induction

**Strong induction** is a variant where the inductive step assumes the result holds for **all values ≤ k**, rather than just `k`.

Useful when the proof for `k + 1` depends on **multiple previous values**, not just the immediate predecessor.

#### Example: Every integer ≥ 2 can be factored into primes

* Base case: 2 is a prime ✅
* Inductive step: assume all integers ≤ `k` can be written as a product of primes.

  * If `k + 1` is prime → done
  * If composite → can be written as `a × b`, where `a, b ≤ k` → both factorable by hypothesis

---

### Loop Invariants

A **loop invariant** is a condition that remains true **before and after each iteration** of a loop.

Used to **formally verify** the correctness of loops.

#### Proving with Loop Invariants:

1. **Initialization**: Show the invariant is true before the first iteration.
2. **Maintenance**: Show that if the invariant is true before an iteration, it remains true after.
3. **Termination**: When the loop finishes, the invariant combined with the exit condition implies correctness.

---

#### Example: Insertion Sort

**Invariant**: At the start of each iteration `i`, the subarray `A[0..i-1]` is sorted.

* **Initialization**: `i = 1`, `A[0..0]` is trivially sorted ✅
* **Maintenance**: We insert `A[i]` into the sorted part `A[0..i-1]`, keeping it sorted ✅
* **Termination**: When `i = n`, the whole array `A[0..n-1]` is sorted ✅

Thus, the loop invariant proves the algorithm’s correctness.

---

#### Advanced Use: Binary Search

**Invariant**: The target value, if it exists, must lie within the current search interval `[low, high]`.

* Carefully update `low` and `high` so that this invariant holds.
* Upon termination, check if the interval is empty or if the mid-value is the target.

Loop invariants can be combined with **assertions** in real code (e.g., using `assert` in Python) to make algorithms more robust and easier to verify.