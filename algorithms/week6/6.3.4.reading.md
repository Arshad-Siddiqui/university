# Reading: 12.4 Studying sorting through an Algorithmic lense

A lot of the algorithms seen are O(nlogn). This is because for sorting the main tool we have is comparison and we have a minimum of number of comparisons necessary to get a sorted list.

As this is the best case scenario for sorting algorithms we say Ω(nlogn)

Omega meaning at minimum we're doing nlogn number of comparisons and likely to do more.

## Trying to beat Ω(nlogn)

Basically when we're able to sort without using comparisons.

### Bucket Sort

When each element keyed and we're trying to get the keys in order we can employ **bucket sort** which is O(n).

Jist of it is we put elements into a bucket array into their correct place and put them back in the original array. This avoids comparisons.

Radix sorting is where you do multiple bucket sorts using different parts of the key. For example with integer keys you could compare units, 10s, 100s per round of bucket sort or with a string based key you could compare each character.

### Stable Sorting

Does it maintain the order of equal keys as they came in and how consistently does it give us the same order. 

This is fine as long as data is handled in queues like in bucket sort.

## 12.5 Comparing Sorting Algorithms

Some sorts run O(^2^)

- Insertion Sort
- Selection Sort

Some run in O(nlogn)

- Heap
- Merge
- Quick

We can have sorting in linear time depending on certain conditions

- Bucket
- Radix

Overall we can just say selection sort is bad as it is O(n^2^) even in it's best cast. The remaining algorithms all have their uses depending on context.

### Insertion Sort

Is also O(n + m) where m is the number of pairs of elements out of order.

This is great where we have an almost sorted sequence as it has the potential to be one of the fastest.

Also as it is really simple to program it is great for relatively small sequences where there aren't many inversionns to deal with.

Overall very useful in these special contexts but in a lot of cases gets outclasses.

### Heap Sort

O(n log n) in the worst case which is fine for a comparison based algorithm.

This can be performed in place so is suited for systems where memory is limited. This pairs with smal to medium datasets where it can all fit onto memory.

This algorithm is unsuited for data sets that exceed memory.

Also this sort is **unstable** due to its nature of swapping elements around.

### Quick Sort

O(n^2^) in the worst case makes it relatively inconsistent in performance when it comes to real time applications.

It isn't very stable but can outperform heap and merge sort on many test.

### Merge Sort

O(nlogn) in the worst case. It is also challenging to get running in place so can be ill-suited for sequences that won't fit on the computer's memory.

However it is really good at making use of a computer's varying memory speeds (cache, ram etc)

### Bucket and Radix Sort

O(d(n+N))

d is the bucket sort algorithm (in this case = 1)
n is the number of values
N is the range of values

Excellent choice when applicable where wee're sorting small integer keys, character strings or tuples from a discrete range.

This is great for smaller sets but larger sets might get benefit from nlogn functions instead.

## Python's built in sorting function

Python can sort by comparing all elements of a list with <.

There is also a way to hand off a function to allow custom sorting.

# 6.4 A little exercise revising duplicate detection problem

1)
```
def detect_duplicates(data):
  sorted_data = binary_sorted(data)
  for i from 1 to length(sorted_data - 1):
    if sorted_data[i] == sorted_data[i-1]
      return false
  return true
```
- Some bad pseudo code because I use a function that I haven't defined.
- I also named the function wrong as binary sort isn't a real thing. I must have been confused a bit with binary search which doesn't do what I want.
- If i wanted to just quickly tweak it and have it be good then I'd just replace 'binary_sorted' with an nlogn time complexity sort such as quicksort or merge sort. That way overall it'd be nlogn which is fine.

2 & 3)
```
def detect_duplicates(data):
  non_dupe_set = set(data)
  non_dupe_data = list(non_dupe_set)
  return non_dupe_data == data
```
The set data type which doesn't hold any duplicates can be used to remove any duplicates. I can then compare the list with duplicates to the list without and return that as the answer.

4)
In terms of time complexity sets use a hashmap where most operations occur in O(1).

But turning a list into a set is O(n) as you're iterating through it to insert each element into the underlying hash table.

Still better than nlogn from previous.

5)
I assume each bit element of data can be compared.